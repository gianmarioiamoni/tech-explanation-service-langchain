# RAG (Retrieval Augmented Generation) Architecture

## ğŸ“‹ Overview

This module implements a complete RAG system for context-aware technical explanations using LangChain LCEL (LangChain Expression Language).

## ğŸ”„ Logic Flow

```
User Topic
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAGService.explain_topic()  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Check: Documents Uploaded?  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“                    â†“
  YES                  NO
   â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retrieve Chunks  â”‚  â”‚ Generic LLM Chain   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ (tech_explanation)  â”‚
   â†“                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Relevant Chunks? â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“          â†“
  YES        NO
   â†“          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG    â”‚  â”‚ Generic LLM Chain   â”‚
â”‚ Chain  â”‚  â”‚ (tech_explanation)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“                    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Sanitized Output         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ Architecture

### Core Components

#### 1. **RAGIndexer** (`rag_indexer.py`)
Handles document ingestion and vectorization.

**Responsibilities:**
- Load documents (PDF, TXT, DOCX)
- Split into chunks
- Create embeddings (OpenAI)
- Store in Chroma vectorstore

**Key Methods:**
- `load_documents(paths)` - Load files
- `split_documents(docs)` - Chunk text
- `add_documents(chunks)` - Index in vectorstore
- `retrieve(query, top_k)` - Semantic search
- `clear()` - Delete all indexed docs

---

#### 2. **RAGRetrieverService** (`rag_retriever.py`)
LCEL-compatible retriever for semantic search.

**Responsibilities:**
- Retrieve relevant document chunks
- Return as LCEL Runnable

**Key Methods:**
- `invoke(query)` - Retrieve documents
- `retrieve_runnable()` - Return as `RunnableLambda` for LCEL chains

---

#### 3. **RAG Chains** (`rag_chains_lcel.py`)
LCEL chains for RAG strategies.

**Strategies:**

##### **Document Stuffing** (Default)
```python
{
    "context": topic â†’ retriever â†’ format_docs,
    "topic": topic (passthrough)
}
â†’ rag_prompt
â†’ llm
â†’ sanitize
```

##### **Map-Reduce**
```python
topic â†’ retrieve docs
â†’ MAP: process each doc separately
â†’ REDUCE: combine results
â†’ sanitize
```

---

#### 4. **RAGService** (`rag_service.py`)
High-level orchestrator for RAG operations.

**Responsibilities:**
- Decide: RAG vs Generic LLM
- Execute appropriate chain
- Manage document lifecycle

**Key Methods:**
- `explain_topic(topic, strategy)` - Main entry point
- `add_document(file_path)` - Upload & index file
- `clear_index()` - Remove all docs
- `has_documents()` - Check if docs exist

---

## ğŸ”— LCEL Chain Details

### Base Chains

#### **Generic LLM Chain** (`tech_explanation_chain.py`)
```python
Input: {"topic": str}
Output: str

prompt | llm | StrOutputParser
```

#### **RAG-Enhanced Chain** (`rag_explanation_chain.py`)
```python
Input: {"topic": str, "context": str}
Output: str

rag_prompt | llm | StrOutputParser
```

### RAG Strategies

#### **Document Stuffing Chain**
Combines all retrieved docs into a single context.

```python
Input: {"topic": str}
Flow:
  1. topic â†’ retriever â†’ format_docs â†’ "context"
  2. topic (passthrough) â†’ "topic"
  3. {topic, context} â†’ rag_chain â†’ sanitize
Output: str (sanitized)
```

**LCEL Implementation:**
```python
{
    "context": RunnableLambda(extract_topic) | retriever | RunnableLambda(format_docs),
    "topic": RunnableLambda(extract_topic)
}
| rag_explanation_chain
| RunnableLambda(sanitize)
```

#### **Map-Reduce Chain**
Processes each doc separately, then combines.

```python
Input: {"topic": str}
Flow:
  1. topic â†’ retriever â†’ docs
  2. MAP: for each doc â†’ rag_chain(topic, doc)
  3. REDUCE: combine all results
  4. sanitize
Output: str (sanitized)
```

---

## ğŸ¯ Usage Examples

### 1. Add Documents
```python
from app.services.rag import RAGService

rag = RAGService()
rag.add_document("path/to/doc.pdf")
```

### 2. Explain with RAG
```python
# Automatically decides: RAG or Generic
result = rag.explain_topic("What is Docker?")
```

### 3. Choose Strategy
```python
# Document stuffing (default)
result = rag.explain_topic("Kubernetes", strategy="document_stuff")

# Map-reduce
result = rag.explain_topic("Kubernetes", strategy="map_reduce")
```

### 4. Clear Index
```python
rag.clear_index()
```

---

## ğŸ“¦ Dependencies

- `langchain-chroma` - Vectorstore
- `langchain-openai` - Embeddings & LLM
- `langchain-core` - LCEL operators
- `langchain-community` - Document loaders
- `langchain-text-splitters` - Text chunking

---

## ğŸ”§ Configuration

### Vectorstore
- **Type**: Chroma
- **Persist**: `./chroma_db/`
- **Embeddings**: `text-embedding-3-small`

### LLM
- **Model**: `gpt-4o-mini`
- **Temperature**: `0.2`
- **Streaming**: `True`

### Chunking
- **Chunk Size**: `500` tokens
- **Chunk Overlap**: `50` tokens

---

## ğŸš€ Integration with UI

See `ui/callbacks/upload_callbacks.py` for Gradio integration:
- `upload_documents()` - File upload handler
- `clear_rag_index()` - Clear button handler

---

## âœ… Best Practices

1. **Always check `has_documents()`** before assuming RAG is available
2. **Use appropriate strategy**:
   - `document_stuff` for consolidated context
   - `map_reduce` for diverse/conflicting sources
3. **Clear index** when switching domains/topics
4. **Monitor chunk quality** - adjust `chunk_size` if needed

---

## ğŸ“ Notes

- Empty vectorstore â†’ Automatic fallback to generic LLM
- No relevant chunks â†’ Automatic fallback to generic LLM
- All prompts configured to **avoid Markdown** (plain text only)
- Output is **always sanitized** before returning

